{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNX4TCWdJNNBqnfyCR5yo9a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task 2 - Classic CV - Drone navigation"],"metadata":{"id":"JjjCPQ6LCIo9"}},{"cell_type":"markdown","source":["For drone navigation and mapping tasks, choosing the right algorithms for image processing is essential. I started with a simple baseline approach by converting all images to grayscale and matching them directly, **without interpolation or smoothing**.\n","\n","Here, I decided to use feature extraction, matching, and geometric transformations (**Homography**) due to their strong suitability for aerial imagery analysis.\n","\n","### **Feature Extraction and Matching (SIFT + FLANN)**\n","\n","I chose **SIFT** (Scale-Invariant Feature Transform) because drone images often have varying scales, rotations, and lighting conditions. SIFT creates special \"keypoints\" (distinctive image features) that stay consistent even under these changes.\n","\n","To quickly and accurately match these keypoints between drone images and the global map, I used **FLANN** (Fast Library for Approximate Nearest Neighbors). FLANN is efficient at matching large sets of descriptors.\n","\n","**Pros of SIFT + FLANN:**\n","- robust to rotation, scaling, and illumination changes;\n","- highly accurate matching;\n","- well-supported and widely used;\n","- FLANN is fast enough even for Google Colab.\n","\n","**Cons of SIFT + FLANN:**\n","- computationally heavier compared to simpler binary descriptors like ORB or AKAZE;\n","- less ideal if real-time speed is critical.\n","\n","Despite the higher computation, SIFT offered the accuracy needed to reliably reconstruct the drone's flight path.\n","\n","### **Geometric Matching (Homography + RANSAC)**\n","\n","After feature matching, I applied **Homography** (a transformation mapping points from one image onto another) to precisely place drone images on the satellite map. For calculating homography, I used **RANSAC** (Random Sample Consensus), which filters out incorrect matches (\"outliers\") and gives reliable results.\n","\n","**Pros of Homography + RANSAC:**\n","- robust against incorrect matches and noise (Fischler & Bolles, 1981);\n","- provides precise position and orientation information;\n","- highly effective for aerial image alignment.\n","\n","**Cons of Homography + RANSAC:**\n","- accuracy strongly depends on the quality of initial matches;\n","- increased computational load if many incorrect matches are present.\n","\n","Overall, combining **SIFT, FLANN, and RANSAC Homography** provided the best balance of robustness, accuracy, and efficiency, suitable for performing this UAV navigation task within the constraints of Google Colab."],"metadata":{"id":"kuCwrGJ7C25r"}},{"cell_type":"markdown","source":["## 0. Libraries"],"metadata":{"id":"GKq9FY6w2DBi"}},{"cell_type":"code","source":["import cv2\n","import os\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re"],"metadata":{"id":"nB4YFdhLvvJt","executionInfo":{"status":"ok","timestamp":1744715239483,"user_tz":-180,"elapsed":234,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## 1. Define paths and load data"],"metadata":{"id":"nvFpTU-E2Kd5"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","global_map_path = '/content/drive/My Drive/Colab Notebooks/drone_navigation_project/data/global_map.png'\n","crops_folder = '/content/drive/My Drive/Colab Notebooks/drone_navigation_project/data/crops/'\n","output_folder = '/content/drive/My Drive/Colab Notebooks/drone_navigation_project/output/'\n","\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Load the global map and create a grayscale version.\n","global_map = cv2.imread(global_map_path)\n","global_map_gray = cv2.cvtColor(global_map, cv2.COLOR_BGR2GRAY)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-vB7NlBvvEe","executionInfo":{"status":"ok","timestamp":1744715267601,"user_tz":-180,"elapsed":28123,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}},"outputId":"9446aa64-6e25-4c1a-cb89-c5816dd558b5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Extract crop number from file name."],"metadata":{"id":"nPl7AR-J2UPk"}},{"cell_type":"code","source":["def get_crop_number(file_path):\n","    filename = os.path.basename(file_path)\n","    numbers = re.findall(r'\\d+', filename)\n","    return int(numbers[0]) if numbers else -1\n","\n","# Load and correctly sort all crop images\n","crop_files = sorted(glob.glob(os.path.join(crops_folder, '*.png')), key=get_crop_number)\n","drone_crops = [cv2.imread(file) for file in crop_files]"],"metadata":{"id":"KFceIv1p0g42","executionInfo":{"status":"ok","timestamp":1744715299181,"user_tz":-180,"elapsed":31583,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 2. Setup SIFT and FLANN matcher for feature detection.\n"],"metadata":{"id":"T63DKhnl2b6G"}},{"cell_type":"markdown","source":["I chose SIFT because its scale- and rotation-invariant features work well under varying conditions, which is essential for UAV photos. I compute the keypoints and descriptors on the global map once and then use FLANN for matching since it is both efficient and effective for high-dimensional descriptor spaces. I also set a minimum match count and defined a margin for later visualization"],"metadata":{"id":"e1Sn6W994uij"}},{"cell_type":"code","source":["# Initialize SIFT detector\n","sift = cv2.SIFT_create()\n","# Compute keypoints and descriptors for the global map\n","kp_global, des_global = sift.detectAndCompute(global_map_gray, None)\n","\n","# Matching parameters:\n","MIN_MATCH_COUNT = 10\n","FLANN_INDEX_KDTREE = 1\n","index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n","search_params = dict(checks=50)\n","# Create the FLANN based matcher\n","flann = cv2.FlannBasedMatcher(index_params, search_params)\n","\n","# Margin (in pixels) to enlarge the region for visualization\n","margin = 50"],"metadata":{"id":"VMGp7nk1vvB1","executionInfo":{"status":"ok","timestamp":1744715312350,"user_tz":-180,"elapsed":13165,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## 3. Prepare for trajectory video output"],"metadata":{"id":"egY6g4as2jeQ"}},{"cell_type":"markdown","source":["To visualize the UAV's flight path, I need to store the drone positions computed for each crop image. I also set up the video writer to create an output video. I use the dimensions of the global map for the video frames and choose a suitable frame rate"],"metadata":{"id":"d4lepFXX4zCr"}},{"cell_type":"code","source":["drone_positions = []  # List to store computed drone positions.\n","\n","frame_height, frame_width = global_map.shape[:2]\n","fps = 1\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","video_output_file = os.path.join(output_folder, 'drone_trajectory.avi')\n","video_writer = cv2.VideoWriter(video_output_file, fourcc, fps, (frame_width, frame_height))\n","\n","# Define a fixed frame size for the zoomed-in video\n","zoom_frame_size = (600, 600)\n","# Create the video writer for the zoomed region video\n","zoom_video_output_file = os.path.join(output_folder, 'drone_trajectory_zoomed.avi')\n","zoom_video_writer = cv2.VideoWriter(zoom_video_output_file, fourcc, fps, zoom_frame_size)"],"metadata":{"id":"gym3AcjIvu_Q","executionInfo":{"status":"ok","timestamp":1744715313169,"user_tz":-180,"elapsed":814,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## 4. Process each crop image: matching, homography, rotation, similarity, visualization, and video frame creation."],"metadata":{"id":"7DJ--oXA2tpO"}},{"cell_type":"markdown","source":["This step is the core of the pipeline. Each crop image taken by the drone is processed to determine its position and orientation on the global satellite map.\n","\n","Result:\n","- drone_trajectory - the full trajectory on the global map, with a triangle indicating the drone's heading;\n","- drone_trajectory_zoomed - a zoomed-in view that shows only the local map region where the drone is in that frame.\n","\n"],"metadata":{"id":"uolRziEm5Td0"}},{"cell_type":"markdown","source":["### Draw a rotated triangle representing the drone."],"metadata":{"id":"p_BQ31O-3GgO"}},{"cell_type":"code","source":["def draw_drone_triangle(frame, center, angle_deg, size=20, color=(0,255,255)):\n","    \"\"\"\n","    Draws a filled triangle on frame with its tip at the given center,\n","    rotated by angle_deg. Angle 0 means the triangle (by default)\n","    points upward.\n","\n","    Parameters:\n","      - frame: the image on which to draw.\n","      - center: (x, y) tuple of the drone's location.\n","      - angle_deg: rotation angle in degrees.\n","      - size: controls the triangle's size.\n","      - color: color for the triangle.\n","    \"\"\"\n","    # Define an isosceles triangle in local coordinates\n","    pts = np.array([[0, -size],\n","                    [-size // 2, size // 2],\n","                    [ size // 2, size // 2]], dtype=np.float32)\n","\n","    theta = np.radians(angle_deg)\n","    R = np.array([[np.cos(theta), -np.sin(theta)],\n","                  [np.sin(theta),  np.cos(theta)]])\n","    pts_rot = np.dot(pts, R.T)\n","    pts_trans = pts_rot + np.array(center, dtype=np.float32)\n","    pts_trans = np.int32(pts_trans).reshape((-1, 1, 2))\n","    cv2.fillPoly(frame, [pts_trans], color)\n","\n","    return frame"],"metadata":{"id":"CiLcD0WgvvHS","executionInfo":{"status":"ok","timestamp":1744715313186,"user_tz":-180,"elapsed":14,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Each crop image is processed one by one in sorted order so that the UAV's flight path appears in the correct sequence. Every crop is converted to grayscale to simplify the feature detection process and reduce computation, since color information isn’t needed for matching.\n","\n","To identify where each crop fits into the global map, keypoints and descriptors are extracted using the SIFT algorithm.\n","\n","FLANN is used to match crop features with those of the global map, since it performs well with high-dimensional data and large descriptor sets. Lowe’s ratio test is applied to remove unreliable matches and keep only the best ones.\n","\n","Once good matches are collected, a homography is calculated using RANSAC to project the crop onto the global map. This step helps determine both the position and orientation of the drone at the time the image was captured. From the homography, the rotation angle is computed using the top edge of the crop image.\n","\n","The match quality is evaluated by calculating the percentage of inliers from the RANSAC result, which helps assess how confident the model is about the match.\n","\n","For visualization, a zoomed-in region around the projected crop location is extracted from the global map. This region is annotated with the crop's contour so it’s easy to see how well it aligns. The center of this region is used to track the UAV’s position over time."],"metadata":{"id":"GiUxeS9d_1Ib"}},{"cell_type":"code","source":["# Process each crop image: matching, homography, rotation, similarity, visualization, and video frame creation\n","for idx, (crop_path, crop) in enumerate(zip(crop_files, drone_crops)):\n","\n","    crop_number = idx  # Use the sorted index as the crop number\n","    crop_gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n","    kp_crop, des_crop = sift.detectAndCompute(crop_gray, None)  # Detect crop features\n","\n","    # Find matches between crop and global map using FLANN with k=2\n","    matches = flann.knnMatch(des_crop, des_global, k=2)\n","    good_matches = []\n","    for m, n in matches:\n","        if m.distance < 0.7 * n.distance:\n","            good_matches.append(m)\n","\n","    print(f\"Crop {crop_number}: {len(good_matches)} good matches found.\")\n","\n","    if len(good_matches) > MIN_MATCH_COUNT:\n","        # Prepare matching points for homography calculation\n","        src_pts = np.float32([kp_crop[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","        dst_pts = np.float32([kp_global[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n","        # Compute the homography matrix using RANSAC to filter outliers\n","        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n","\n","        # Define the four corners of the crop image and transform them using homography\n","        h_crop, w_crop = crop_gray.shape\n","        pts = np.float32([[0, 0],\n","                          [0, h_crop - 1],\n","                          [w_crop - 1, h_crop - 1],\n","                          [w_crop - 1, 0]]).reshape(-1, 1, 2)\n","        dst = cv2.perspectiveTransform(pts, H)\n","\n","        # Compute the rotation angle based on the transformation of the top edge\n","        pt_origin = np.array([[0, 0]], dtype=np.float32).reshape(1, 1, 2)\n","        pt_x = np.array([[w_crop - 1, 0]], dtype=np.float32).reshape(1, 1, 2)\n","        trans_origin = cv2.perspectiveTransform(pt_origin, H)[0][0]\n","        trans_pt_x = cv2.perspectiveTransform(pt_x, H)[0][0]\n","        dx = trans_pt_x[0] - trans_origin[0]\n","        dy = trans_pt_x[1] - trans_origin[1]\n","        angle_rad = np.arctan2(dy, dx)\n","        angle_deg = np.degrees(angle_rad)\n","\n","        # Calculate similarity percentage from inlier ratio\n","        if mask is not None and len(mask) > 0:\n","            similarity = (np.sum(mask) / len(mask)) * 100\n","        else:\n","            similarity = 0.0\n","\n","        # Create a zoomed region for side-by-side visualization by finding a bounding rectangle\n","        x, y, w, h = cv2.boundingRect(np.int32(dst))\n","        x_zoom = max(x - margin, 0)\n","        y_zoom = max(y - margin, 0)\n","        x_end = min(x + w + margin, global_map.shape[1])\n","        y_end = min(y + h + margin, global_map.shape[0])\n","        zoomed_region = global_map[y_zoom:y_end, x_zoom:x_end].copy()\n","        dst_adjusted = dst - np.array([[x_zoom, y_zoom]], dtype=np.float32)\n","        zoomed_region_with_contour = cv2.polylines(zoomed_region.copy(), [np.int32(dst_adjusted)], True, (0, 255, 0), 3)\n","\n","        # Compute the center (drone position) and update our trajectory list\n","        center = np.mean(dst, axis=0)\n","        center_tuple = (int(center[0][0]), int(center[0][1]))\n","        drone_positions.append(center_tuple)\n","\n","        # Create side-by-side visualization for review (crop + zoomed global map)\n","        fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n","        axs[0].imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n","        axs[0].set_title(f\"Crop {crop_number}\")\n","        axs[0].axis(\"off\")\n","        axs[1].imshow(cv2.cvtColor(zoomed_region_with_contour, cv2.COLOR_BGR2RGB))\n","        axs[1].set_title(\"Zoomed Global Map\")\n","        axs[1].axis(\"off\")\n","        axs[1].text(10, 20, f\"Rotation: {angle_deg:.1f}°\", color='red', fontsize=14, backgroundcolor='white')\n","        axs[1].text(10, 35, f\"Similarity: {similarity:.1f}%\", color='red', fontsize=14, backgroundcolor='white')\n","        plt.tight_layout()\n","        output_file = os.path.join(output_folder, f'combined_zoom_output_{crop_number}.png')\n","        plt.savefig(output_file, bbox_inches='tight')\n","        plt.show()\n","\n","        # Create the global map video frame\n","        frame = global_map.copy()\n","        if len(drone_positions) > 1:\n","            for i in range(1, len(drone_positions)):\n","                cv2.line(frame, drone_positions[i-1], drone_positions[i], (0, 0, 255), 2)\n","            cv2.arrowedLine(frame, drone_positions[-2], drone_positions[-1], (255, 255, 0), 2, tipLength=0.3)\n","        frame = draw_drone_triangle(frame, center_tuple, angle_deg, size=20, color=(0, 255, 255))\n","        cv2.putText(frame, f\"Rotation: {angle_deg:.1f} deg\", (center_tuple[0] + 15, center_tuple[1] + 30),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n","        cv2.putText(frame, f\"Similarity: {similarity:.1f}%\", (center_tuple[0] + 15, center_tuple[1] + 55),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n","        cv2.putText(frame, f\"Frame {crop_number}\", (center_tuple[0] + 15, center_tuple[1] - 15),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n","        video_writer.write(frame)  # Write the global frame to the video\n","\n","        # Create the zoomed video frame by resizing the zoomed region to a fixed size\n","        zoom_frame = cv2.resize(zoomed_region_with_contour, zoom_frame_size)\n","        zoom_video_writer.write(zoom_frame)  # Write the zoomed frame to the zoomed video\n","\n","    else:\n","        print(f\"Not enough matches found for crop {crop_number} ({len(good_matches)} good matches).\")\n","\n","video_writer.release()\n","zoom_video_writer.release()\n","print(f\"Global video saved to {video_output_file}\")\n","print(f\"Zoomed video saved to {zoom_video_output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1-cSbSKdYVoMOmDDABHZxqYRWtdMD_Vs2"},"id":"6b2RjJzbhNiM","executionInfo":{"status":"ok","timestamp":1744715577982,"user_tz":-180,"elapsed":264794,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}},"outputId":"1fbb7d6e-0e59-408d-8b6b-4f0882a8fad0"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## 5. Visualized Route Map"],"metadata":{"id":"01hlmdlYIEil"}},{"cell_type":"markdown","source":["Create an extra image that shows the global map overlaid with the full drone route. Draw a polyline connecting the route points, marks each point with a small circle and order number, and explicitly label the start and finish positions."],"metadata":{"id":"CW813mMkIP-I"}},{"cell_type":"code","source":["# Copy the original global map to avoid overwriting it.\n","route_map = global_map.copy()\n","\n","if drone_positions:\n","    # Draw the complete route as a polyline connecting all drone positions\n","    pts = np.array(drone_positions, dtype=np.int32)\n","    cv2.polylines(route_map, [pts], isClosed=False, color=(0, 0, 255), thickness=2)\n","\n","    # Loop through each drone position to mark and annotate the order\n","    for idx, pos in enumerate(drone_positions):\n","        # Draw a small circle at the route point\n","        cv2.circle(route_map, pos, radius=5, color=(255, 0, 0), thickness=-1)\n","        # Annotate with the order number near the circle\n","        cv2.putText(route_map, str(idx), (pos[0] + 10, pos[1] + 10),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), thickness=2, lineType=cv2.LINE_AA)\n","\n","    # Mark the start point with a label 'Start'\n","    start_point = drone_positions[0]\n","    cv2.putText(route_map, \"Start\", (start_point[0] - 20, start_point[1] - 10),\n","                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), thickness=3, lineType=cv2.LINE_AA)\n","\n","    # Mark the finish point with a label 'Finish'\n","    finish_point = drone_positions[-1]\n","    cv2.putText(route_map, \"Finish\", (finish_point[0] - 20, finish_point[1] - 10),\n","                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), thickness=3, lineType=cv2.LINE_AA)\n","\n","    # Save the visualized route image to the output folder\n","    route_output_file = os.path.join(output_folder, 'route_overview.png')\n","    cv2.imwrite(route_output_file, route_map)\n","    print(f\"Route overview image saved to {route_output_file}\")\n","else:\n","    print(\"No drone positions were detected. Please ensure that the crop processing step produced valid positions.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DirHuSH_LgvO","executionInfo":{"status":"ok","timestamp":1744716121852,"user_tz":-180,"elapsed":759,"user":{"displayName":"Alex Danvers","userId":"17068446901206526210"}},"outputId":"8cdb11d5-101e-4775-8b69-6a79c5e3e510"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Route overview image saved to /content/drive/My Drive/Colab Notebooks/drone_navigation_project/output/route_overview.png\n"]}]},{"cell_type":"markdown","source":["## What could be improved in future\n","\n","- the trajectory could be smoothed using interpolation or filtering to remove jitter and better reflect real drone motion;\n","- matching could be improved by combining SIFT with other features like color histograms or semantic segmentation;\n","- instead of homography from a single frame, temporal information across frames could be used to improve robustness;\n","- adding support for handling frames with too few matches by predicting position from past trajectory would help fill gaps;\n","- the zoomed-in region size could adapt dynamically based on match quality or drone altitude;\n","- exporting data in formats like GeoJSON or CSV could help with further analysis or GIS integration."],"metadata":{"id":"wGppP5UPBn2j"}}]}