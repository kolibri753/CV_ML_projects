# Titanic Survival: Neural Network Training & Monitoring

## Теоретичні концепції

### BackPropagation  
BackPropagation — алгоритм навчання багатошарової нейронної мережі. Основні етапи:  
1. **Пряме поширення**: вхідні ознаки проходять через шари, генеруючи вихід.  
2. **Обчислення помилки**: різниця між прогнозом і істинною міткою.  
3. **Зворотне поширення**: обчислення градієнтів помилки по відношенню до ваг кожного шару.  
4. **Оновлення ваг**: застосування градієнтного спуску (або його варіантів) для мінімізації функції втрат.

### EarlyStopping  
Метод регуляризації, що припиняє тренування, якщо валідаційна втрата не покращується протягом заданої кількості епох. Забезпечує:  
- автоматичну зупинку до явного переобучення;  
- повернення до найкращих ваг, зафіксованих до плато або росту валідаційної втрати.

### DropOut  
Регуляризаційна техніка випадкового “вимкнення” частини нейронів у прихованих шарах під час кожного кроку навчання.  
- Зменшує взаємозалежність вузлів (co-adaptation).  
- Підвищує здатність мережі узагальнювати, зменшуючи variance.

---

## Короткий опис виконаних кроків

1. **Об’єднання та передобробка даних**  
   - Завантаження Train/Test CSV, заповнення пропусків (`Age` за медіанами Title+Pclass; `Embarked` — модою).  
   - Інженерія ознак: кодування `Sex`, виділення і кодування `Title`; логарифмування `Fare`; one-hot для `Cabin`, `Ticket`, `Embarked`, `Pclass`.  

2. **Побудова трьох архітектур**  
   - **No DropOut**: двошарова мережа (128 → 64) без регуляризації.  
   - **More Layers**: глибша мережа (256 → 128 → 64) для оцінки впливу глибини на bias/variance.  
   - **With DropOut**: двошарова архітектура з DropOut(0.5) після кожного шару.

3. **Налаштування тренування**  
   - **Оптимізатор**: Adam  
   - **Функція втрат**: Binary Cross-Entropy  
   - **Метрика**: Accuracy  
   - **Callbacks**:  
     - EarlyStopping (monitor=val_loss, patience=5, restore_best_weights).  
     - TensorBoard (логування історії loss/accuracy для кожної моделі).

4. **Автоматичний пошук гіперпараметрів**  
   - Використано Hyperband (Keras Tuner) для підбору: кількості шарів, нейронів, активацій, DropOut-rate, learning rate.  
   - Отримана оптимальна конфігурація: одна прихована мережа (160→32→32), комбінація активацій (tanh, relu, elu), DropOut на рівнях 0.1/0.5, lr≈2.6e-3.

5. **Моніторинг та порівняння**  
   - Зафіксовано для кожної моделі: train/validation accuracy та loss на останній епосі.  
   - Побудовано графіки зміни loss/accuracy по епохах, що дозволило візуально оцінити момент появи переобучення та ефективність DropOut.

---

## Результати

| Model Name       | Epochs | Train Acc | Val Acc | Train Loss | Val Loss |
|------------------|:------:|:---------:|:-------:|:----------:|:--------:|
| No DropOut       |   15   |   0.830   |  0.816  |   0.3875   |  0.4277  |
| More Layers      |   12   |   0.851   |  0.799  |   0.3784   |  0.4535  |
| With DropOut     |   50   |   0.843   |  0.827  |   0.3963   |  0.4390  |
| Hyperband Tuned  |   14   |   0.824   |  0.832  |   0.4025   |  0.4523  |

- **Глибока модель** досягла найнижчого train loss, але страждала від сильного overfitting.  
- **Модель з DropOut** показала найкращий баланс, зменшивши розрив між train/val метриками.  
- **Hyperband-модель** конвергувала швидко (~14 епох) та досягла найвищої валідаційної точності.

---

## Висновки

- **BackPropagation** дозволяє точно мінімізувати loss через обчислення градієнтів у всі шари.  
- **EarlyStopping** оперативно зупиняє тренування до різкого підйому variance.  
- **DropOut** суттєво знижує ризик переобучення, особливо у глибоких архітектурах.  
- **Автоматичний tuner** (Hyperband) швидко знаходить ефективні поєднання плоскої та глибокої структури мережі, оптимізуючи learning rate і рівень регуляризації.

Загалом, комбінація цих методів — зворотного поширення, ранньої зупинки та DropOut — разом із автоматизованим пошуком гіперпараметрів забезпечує ефективне навчання нейронної мережі з високою узагальнювальною здатністю.
