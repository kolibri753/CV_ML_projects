# CNN: Image Classification

Тут було реалізовано кілька ключових експериментів зі згортковими нейронними мережами:

1. **MNIST**  
   - Модель: `lab7/my_model_cnn.keras`  
   - Застосовано до власноруч намальованих зображень із папки `lab7/numbers`  

2. **Fashion-MNIST**  
   - Модель: `lab7/fashion_mnist_cnn.keras`  

3. **CIFAR-10**  
   - Модель: `lab7/cifar10_cnn.keras`  

## Теоретичні відомості про CNN

**Походження та застосування**  
Згорткові нейронні мережі (CNN) виникли на основі досліджень зорової кори головного мозку: місцеві рецептивні поля нейронів реагують на фрагменти зображення (лінії, кути), а глибші шари поєднують ці ознаки в складні патерни. Сьогодні CNN застосовують у комп’ютерному зорі (розпізнавання об’єктів, сегментація, детекція), робототехніці, обробці мови й аудіо.

**Основні компоненти**  
- **Згортковий шар (Conv2D)**  
  – _Рецептивне поле_ (kernel size): зона підключення кожного нейрона, зазвичай 3×3 або 5×5;  
  – _Число фільтрів_ (filters): скільки різних ознак шукається;  
  – _Stride_ (крок): наскільки далеко рухається вікно згортки;  
  – _Padding_ (“same”/“valid”): чи додаються нулі навколо зображення нулі, щоб після згортки вихід мав той самий розмір, що й вхід;
  – _Активація_ (ReLU, LeakyReLU тощо).  

- **Pooling-шар**  
  – Макс-пулінг (MaxPool2D): зменшує розміри, узагальнюючи найбільші значення у вікні (зазвичай 2×2, stride=2);  
  – Служить для інваріантності до невеликих зсувів та для зниження кількості параметрів.

- **Повнозв’язні шари (Dense)**  
  – Агрегують виявлені ознаки для остаточної класифікації;  
  – Dropout-регуляризація для зниження перенавчання.

**Гіперпараметри**  
- Кількість шарів і фільтрів — контролює ємність мережі;  
- Розмір kernel і stride — впливають на роздільність ознак та швидкість обробки;  
- Тип padding — балансує між збереженням розмірів і витратами обчислень;  
- Learning rate, оптимізатор (Adam, RMSprop), регуляризація (L2, Dropout) — визначають швидкість та стабільність збіжності.

**Чому CNN ефективні**  
- **Спільне використання ваг**: один фільтр «прохойдує» по всьому зображенню, що значно зменшує число параметрів;  
- **Локальні зв’язки**: фокус на малих рецептивних полях дозволяє виявляти низькорівневі текстури;  
- **Ієрархія ознак**: крок за кроком прості патерни об’єднуються в дедалі складніші;  
- **Інваріантність**: пулінг і великий receptive field роблять модель стійкою до зсувів, масштабування та деформацій.

---

## Експериментальні висновки

1. **MNIST (>99 % accuracy)**  
   Модель швидко збіглася, показавши інваріантність до варіацій товщини ліній та інвертованого фону.  

2. **Fashion-MNIST (≈92.4 %)**  
   Висока точність на однорідних текстурах, труднощі з «Shirt» vs «T-shirt/top» (recall~72 %).  

3. **CIFAR-10 (≈78.6 %)**  
   Добре класифіковано «ship», «truck» (>88 %), але «cat», «bird» лише в 61–64 % випадків; плутанина між «cat↔dog» і «automobile↔truck».

## Рекомендації для покращення

- Додавати **BatchNormalization** для стабільнішої збіжності  
- Використовувати **залишкові (ResNet) блоки** або **модулі Inception** для збільшення ємності  
- Інтенсивно аугментувати дані (геометрія, колір)  
- Балансувати ваги класів або вводити **функцію втрат зі зважуванням** для «складних» категорій  

---

_Цей набір експериментів ілюструє фундаментальні принципи CNN — локальні зв’язки, спільне використання ваг та ієрархічне виділення ознак — і підкреслює, що для складних кольорових та текстурно-різноманітних наборів необхідні більш глибокі та спеціалізовані архітектури._  
