# Рекурентні нейронні мережі (RNN, LSTM, GRU) для прогнозування часових рядів та генерації 

## 1. Теоретичні засади

### 1.1. Рекурентні нейронні мережі (RNN)  
RNN обробляють послідовні дані, зберігаючи «прихований стан», який несе інформацію про попередні кроки. Однак при великій довжині послідовності страждає затухання (або вибух) градієнта.

### 1.2. LSTM (Long Short-Term Memory)  
LSTM усувають проблему затухання градієнта завдяки трьом гейтам:
- **Forget Gate** — вирішує, які дані «забути».  
- **Input Gate** — визначає, яку нову інформацію додати.  
- **Output Gate** — керує виходом із блока пам’яті.

Це дозволяє моделі утримувати довготривалі залежності.

### 1.3. GRU (Gated Recurrent Unit)  
GRU — спрощений варіант LSTM з двома гейтами:
- **Update Gate** (поєднує функції input+forget)  
- **Reset Gate**  

GRU має менше параметрів, швидше навчається й часто дає близькі за якістю результати.

### 1.4. Регуляризація та моніторинг  
- **EarlyStopping**: зупиняє тренування при відсутності покращення валідаційної втрати.  
- **DropOut** (якщо використовується): випадкове «вимкнення» нейронів під час навчання.  

---

## 2. Проєкт 1: Прогнозування ціни акцій Netflix

### 2.1. Дані і підготовка  
- Джерело: `Netflix_Stock_Price.csv` (1009 торгових днів).  
- Колонки: Date, Open, High, Low, Close, Adj Close, Volume.  
- Ціль: передбачення **Close** (ціни закриття).  
- Нормалізація: `MinMaxScaler(0,1)` для стовпців Open, High, Low, Volume, Close.  
- Формування послідовностей з “look_back” попередніх днів (10 за замовчуванням).  

### 2.2. Архітектури моделей  
- **SimpleRNN**, **LSTM**, **GRU**  
- Випробували по 1–2 шари та 50–100 нейронів у кожному.  
- Універсальна функція `create_rnn_model(type, layers, units)` автоматизує побудову.

### 2.3. Навчання й оцінка  
- Розбили дані 80 / 20 (train/test), метрики: MSE, RMSE, MAE, MAPE, R².  
- **EarlyStopping(monitor=val_loss, patience=5)**.  
- Порівняння за двома підходами:  
  1. **Hold-Out** (одноразовий розподіл)  
  2. **5-Fold K-Fold** — усереднення метрик по фолдах.

#### Ключові результати  
| Модель                   | Hold-Out R² | K-Fold R² |
|--------------------------|:-----------:|:---------:|
| GRU (2 шари × 50)        | 0.9848      | 0.9864    |
| GRU (2 шари × 100)       | 0.9852      | 0.9866    |
| LSTM (2 шари × 50)       | 0.9783      | 0.9784    |
| SimpleRNN (2 шари × 100) | 0.9854      | 0.9850    |

Найкраща узагальнювальна здатність у **GRU (2 шари × 100 нейронів)**.

### 2.4. Візуалізація прогнозу  
Побудовано графік фактичних і прогнозованих цін закриття на часовій осі — добре збігаються, особливо для GRU-конфігурацій.

### 2.5. Дослідження глибини історії (look_back)  
Експериментували з look_back = 5, 10, 15, 20, 30, 50, 100, 500.  
Найнижча помилка виявлена при **look_back=5**, оптимальне балансування контексту та «шуму».

---

## 3. Проєкт 2: Генерація тексту пісень

### 3.1. Дані і передобробка  
- Джерело: CSV із текстами пісень (стовпець “Lyrics”).  
- Очищення: видалення небажаних символів, стандартизація пробілів.  
- Відбір рядків із ≥ 20 словами, випадкова вибірка для аналізу.  
- **Tokenizer** → словник; перетворення тексту в індекси.  
- Створення n-грамних послідовностей із падінгом до єдиної довжини (`pad_sequences`).  
- Формат:  
  - **X**: усі токени, окрім останнього  
  - **y**: останній токен → one-hot.

### 3.2. Архітектури моделей  
- **LSTM** і **GRU**, 1–2 шари, 100–200 нейронів, розмір embedding = 100–200.  
- Функція `create_text_generation_model(total_words, max_len, emb_dim, type, layers, units)`.

### 3.3. Навчання  
- Функція втрат: `categorical_crossentropy`; оптимізатор: `adam`.  
- **EarlyStopping(monitor=loss, patience=5)**.  
- Кожну конфігурацію тренували 30–50 епох.

#### Результати втрат (loss)  
GRU converges faster і до нижчої кінцевої втрати, ніж LSTM у аналогічних умовах.

### 3.4. Генерація тексту  
- Функція `generate_text(seed, next_words, model, max_len, tokenizer)`:  
  1. Токенізує `seed_text`.  
  2. Доповнює до довжини.  
  3. Прогнозує наступний індекс → слово → додає до рядка.  
  4. Повторює `next_words` разів.

- Порівняння генерацій різними моделями на однаковому початку (“And the sky is grey…”).  
  - **GRU (1 шар × 200, emb 150)** дає найбільш «поетичні» та когерентні фрази.  
  - LSTM іноді генерує повтори та менше логіки.

---

## 4. Загальні висновки

1. **RNN vs LSTM vs GRU**  
   - RNN добре для простих послідовностей, але LSTM/GRU краще працюють із довгими залежностями.  
   - GRU простіше, швидше навчається і на малих даних часто дає результат, порівнянний з LSTM.

2. **EarlyStopping**  
   - Дозволяє оптимально зупиняти навчання, захищаючи від переобучення.

3. **Гіперпараметри**  
   - Кількість шарів і одиниць слід обирати залежно від складності даних: надто потужні моделі ризикують «переучитися».  
   - Розмір `look_back` у часових рядах має бути достатнім для вловлення трендів, але не надто великим, щоб уникнути «шуму».

4. **Практичні рекомендації**  
   - Для прогнозування фінансових рядів із чіткими патернами часто достатньо **GRU (2 шари, 100 нейронів)**.  
   - Для генерації тексту варто починати з **GRU (1 шар, ~200 нейронів, embedding ~150)** і корегувати параметри за цільовою задачею.

---

**Ключові інструменти:**  
- TensorFlow / Keras: `Sequential`, `SimpleRNN`, `LSTM`, `GRU`, `Embedding`, `EarlyStopping`  
- Scikit-learn: `MinMaxScaler`, `train_test_split`, `KFold`, метрики (`MSE`, `MAE`, `R²`)  
- Pandas / NumPy для обробки даних, Matplotlib для візуалізації.
